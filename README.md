# krippendorff-alpha-demo
<a href="http://en.wikipedia.org/wiki/Krippendorff's_Alpha">Krippendorff's alpha</a> implementation in python for measuring inter-reliability among 2 or more assessors. Jupyter Notebook includes a 'how to use' demo using a small set.

This is a standard statistic used to evaluate reliability of 'ground truth' training sets for Machine Learning applications created using crowdsourcing methodologies like Amazon's Mechanical Turk.

Values range from 0 to 1, where 0 is perfect disagreement and 1 is perfect agreement. Krippendorff suggests: “It is customary to require α ≥ .800. Where tentative conclusions are still acceptable, α ≥ .667 is the lowest conceivable limit".

# References:
Python code: https://github.com/grrrr/krippendorff-alpha

Krippendorff, K. (2011). “Computing Krippendorff’s alpha-reliability.” Philadelphia: Annenberg School for Communication Departmental Papers. Retrieved July 6, 2011 from: http://repository.upenn.edu/cgi/viewcontent.cgi?article=1043&context=asc_papers
